Build more powerful voice agents with the Gemini Live API 
We've significantly improved function calling and enhanced proactive audio capabilities in the Live API to handle interruptions, pauses and side conversations gracefully.
Ivan Solovyev, Product Manager, Google DeepMind
Valeria Wu, Product Manager, Google DeepMind
Mingqiu Wang, Engineer, Google DeepMind
Today, we’re excited to announce a significant update to the Live API in the Gemini API, featuring a new native audio model, now available in preview. This update is designed to help you build more reliable, responsive, and natural-sounding voice agents.
For this model release, we've focused on two key areas:
More robust function calling: Making it more reliable for your agent to connect to external data and services.
More natural conversations: Ensuring interactions feel more intuitive with a better understanding of the context and natural resumption of the conversation in case of interruptions or pauses.
A Major Boost for Reliability
The most powerful and interesting voice agent experiences are unlocked when they can reliably connect to external data and services—allowing users to access real-time information, book appointments, or complete transactions. This is where function calling comes in. Given the real-time nature of voice interactions, there’s no time to retry a failed request, making the reliability of function calling absolutely critical.
To see what this improved reliability looks like in practice, here’s a quick demo of it in action:
0:01 / 1:31
More reliable function calling
The new model is significantly better at identifying the correct function to call, knowing when not to call a function, and adhering consistently  to the provided tool schema. Our internal benchmarks show a dramatic improvement in function calling accuracy (e.g. the model correctly identifying and calling a function, including in complex scenarios with 10 or more active functions). Compared to the previous version, function calling success increased by 2x in single-call tests and 1.5x in tests involving 5 to 10 calls. This boost in reliability is a big step forward for voice applications and we are continuing to improve reliability, especially for multi-turn scenarios, based on developer feedback.
Test the model’s function calling improvements with this app in Google AI Studio.
Results based on tests run on Google AI Studio and Vertex AI
Even More Natural Conversations
We've also added more proactive audio capabilities to make interactions feel more natural. The model now ignores chatter not relevant to the active conversation and is also significantly better at understanding natural pauses and interruptions by the user.
Imagine you're talking to a voice agent and someone walks into the room to ask you a quick question. The model can now gracefully pause the conversation, while ignoring the side chatter, and seamlessly resume when you're ready to continue.
0:00 / 0:22
Better recognition of background conversations
Similarly, the model is now better at understanding conversational rhythms, recognizing the context of your speech and adapting to your pauses—whether you're taking a moment to articulate a complex thought or just speaking casually. In our internal evaluations, the number of times the model incorrectly interrupts the user when they are not talking dropped significantly compared to the last model. These improvements happen automatically, with no extra setup required, making the conversation much more fluid.
0:01 / 0:41
Graceful handling of natural pauses in the conversation
This update also brings significant improvements in interruptions detection accuracy, noticeably reducing the number of times the model fails to recognize when a user interrupts.
0:00 / 0:34
Significantly improved detection of interruptions
Smarter Responses with "Thinking" Capabilities
As a followup to this launch, next week we are rolling out support for "thinking" capabilities, similar to those in Gemini 2.5 Flash and Pro. We recognize that not all questions can or should be answered instantly. For complex queries that require deeper reasoning, you will be able  set a "thinking budget," allowing the model to take a few moments to process the request more thoroughly. As part of the thinking process, the model will send back a text summary of its thinking.
Live API in the real world
We’ve been working closely with our early access partners to test and improve the API capabilities and almost all have reported positive results from testing the latest model.
For example, Ava, an AI-powered family operating system, uses the Live API to act as a "household COO." Ava processes messy, real-world inputs like school emails, PDFs, and voice notes, turning them into actions like calendar events.
"The ability to have natural, bi-directional voice chat was a hard requirement," says Joe Alicata, Cofounder and CTO of Ava. "The latest model's improvements to function calling accuracy were a game-changer. We're seeing higher first-pass accuracy on noisy inputs and fewer brittle prompt hacks, which allowed our small team to ship a reliable, agentic, multimodal product much faster."
Get Started Today
You can start building with Live API right now:
python
import asyncio
from google import genai
from google.genai import types

client = genai.Client()

model = "gemini-2.5-flash-native-audio-preview-09-2025"

system_instruction = """You are a helpful and friendly AI assistant.
Your default tone is helpful, engaging, and clear, with a touch of optimistic wit.
Anticipate user needs by clarifying ambiguous questions and always conclude your responses with an engaging follow-up question to keep the conversation flowing."""

config = {
    "response_modalities": ["AUDIO"],
    "system_instruction": system_instruction,
}

async def main():
    async with client.aio.live.connect(model=model, config=config) as session:

        # Get audio data, e.g., from microphone
        audio_bytes = record_audio()
        
        # Send audio
        await session.send_realtime_input(
            audio=types.Blob(data=audio_bytes, mime_type="audio/pcm;rate=16000")
        )
        
        # Receive responses
        async for response in session.receive():
            if response.data is not None:
                # Play audio...


if __name__ == "__main__":
    asyncio.run(main())
Head over to the Live API documentation to learn more and find end-to-end code samples in the cookbooks.
We believe these updates will unlock new possibilities for creating powerful and intuitive voice experiences, and we will have even more to share about the Live API soon. Happy building!

Multimodal Live API - Quickstart


Preview: The Live API is in preview.

This notebook demonstrates simple usage of the Gemini Multimodal Live API. For an overview of new capabilities refer to the Gemini Live API docs.

This notebook implements a simple turn-based chat where you send messages as text, and the model replies with audio. The API is capable of much more than that. The goal here is to demonstrate with simple code.

Some features of the API are not working in Colab, to try them it is recommended to have a look at this Python script and run it locally.

If you aren't looking for code, and just want to try multimedia streaming use Live API in Google AI Studio.

The Next steps section at the end of this tutorial provides links to additional resources.

Native audio output
Info: Gemini 2.5 introduces native audio generation, which directly generates audio output, providing a more natural sounding audio, more expressive voices, more awareness of additional context, e.g., tone, and more proactive responses. You can try a native audio example in this script.

Setup
Install SDK
The new Google Gen AI SDK provides programmatic access to Gemini 2.5 (and previous models) using both the Google AI for Developers and Vertex AI APIs. With a few exceptions, code that runs on one platform will run on both.

More details about this new SDK on the documentation or in the Getting started notebook.


[ ]
%pip install -U -q google-genai
Note: you may need to restart the kernel to use updated packages.

[notice] A new release of pip is available: 25.1.1 -> 25.2
[notice] To update, run: python.exe -m pip install --upgrade pip
Set up your API key
To run the following cell, your API key must be stored in a Colab Secret named GOOGLE_API_KEY. If you don't already have an API key, or you're not sure how to create a Colab Secret, see Authentication for an example.


[ ]
from google.colab import userdata
import os

os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')

Initialize SDK client
The client will pick up your API key from the environment variable.


[ ]
from google import genai
from google.genai import types
client = genai.Client(api_key=GOOGLE_API_KEY)
Select a model
The Gemini 2.5 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output.


[ ]
MODEL = "gemini-live-2.5-flash-preview" # @param ["gemini-2.0-flash-live-001", "gemini-live-2.5-flash-preview","gemini-2.5-flash-preview-native-audio-dialog"] {"allow-input":true, isTemplate: true}
MODEL:
gemini-live-2.5-flash-preview

Import
Import all the necessary modules.


[ ]
import asyncio
import base64
import contextlib
import datetime
import os
import json
import wave
import itertools

from IPython.display import display, Audio

from google import genai
from google.genai import types
Text to Text
The simplest way to use the Live API is as a text-to-text chat interface, but it can do a lot more than this.


[ ]
config={
    "response_modalities": ["TEXT"]
}

async with client.aio.live.connect(model=MODEL, config=config) as session:
  message = "Hello? Gemini are you there?"
  print("> ", message, "\n")
  await session.send_client_content(
        turns={"role": "user", "parts": [{"text": message}]}, turn_complete=True
  )

  # For text responses, When the model's turn is complete it breaks out of the loop.
  turn = session.receive()
  async for chunk in turn:
    if chunk.text is not None:
      print(f'- {chunk.text}')
>  Hello? Gemini are you there? 

- Hello
-  there! I am indeed here. How can I help you today?
Simple text to audio
The simplest way to playback the audio in Colab, is to write it out to a .wav file. So here is a simple wave file writer:


[ ]
@contextlib.contextmanager
def wave_file(filename, channels=1, rate=24000, sample_width=2):
    with wave.open(filename, "wb") as wf:
        wf.setnchannels(channels)
        wf.setsampwidth(sample_width)
        wf.setframerate(rate)
        yield wf
The next step is to tell the model to return audio by setting "response_modalities": ["AUDIO"] in the LiveConnectConfig.

When you get a response from the model, then you write out the data to a .wav file.


[ ]
config={
    "response_modalities": ["AUDIO"]
}

async def async_enumerate(aiterable):
  n=0
  async for item in aiterable:
    yield n, item
    n+=1


async with client.aio.live.connect(model=MODEL, config=config) as session:
  file_name = 'audio.wav'
  with wave_file(file_name) as wav:
    message = "Hello? Gemini are you there?"
    print("> ", message, "\n")
    await session.send_client_content(
        turns={"role": "user", "parts": [{"text": message}]}, turn_complete=True
    )

    turn = session.receive()
    async for n,response in async_enumerate(turn):
      if response.data is not None:
        wav.writeframes(response.data)

        if n==0:
          print(response.server_content.model_turn.parts[0].inline_data.mime_type)
        print('.', end='')


display(Audio(file_name, autoplay=True))


Towards Async Tasks
The real power of the Live API is that it's real time, and interruptable. You can't get that full power in a simple sequence of steps. To really use the functionality you will move the send and recieve operations (and others) into their own async tasks.

Because of the limitations of Colab this tutorial doesn't totally implement the interactive async tasks, but it does implement the next step in that direction:

It separates the send and receive, but still runs them sequentially.
In the next tutorial you'll run these in separate async tasks.
Setup a quick logger to make debugging easier (switch to setLevel('DEBUG') to see debugging messages).


[ ]
import logging

logger = logging.getLogger('Live')
logger.setLevel('INFO')
The class below implements the interaction with the Live API.


[ ]
class AudioLoop:
  def __init__(self, turns=None,  config=None):
    self.session = None
    self.index = 0
    self.turns = turns
    if config is None:
      config={
          "response_modalities": ["AUDIO"]}
    self.config = config

  async def run(self):
    logger.debug('connect')
    async with client.aio.live.connect(model=MODEL, config=self.config) as session:
      self.session = session

      async for sent in self.send():
        # Ideally send and recv would be separate tasks.
        await self.recv()

  async def _iter(self):
    if self.turns:
      for text in self.turns:
        print("message >", text)
        yield text
    else:
      print("Type 'q' to quit")
      while True:
        text = await asyncio.to_thread(input, "message > ")

        # If the input returns 'q' quit.
        if text.lower() == 'q':
          break

        yield text

  async def send(self):
    async for text in self._iter():
      logger.debug('send')

      # Send the message to the model.
      await self.session.send_client_content(
        turns={"role": "user", "parts": [{"text": text}]}, turn_complete=True
      )
      logger.debug('sent')
      yield text

  async def recv(self):
    # Start a new `.wav` file.
    file_name = f"audio_{self.index}.wav"
    with wave_file(file_name) as wav:
      self.index += 1

      logger.debug('receive')

      # Read chunks from the socket.
      turn = self.session.receive()
      async for n, response in async_enumerate(turn):
        logger.debug(f'got chunk: {str(response)}')

        if response.data is None:
          logger.debug(f'Unhandled server message! - {response}')
        else:
          wav.writeframes(response.data)
          if n == 0:
            print(response.server_content.model_turn.parts[0].inline_data.mime_type)
          print('.', end='')

      print('\n<Turn complete>')

    display(Audio(file_name, autoplay=True))
    await asyncio.sleep(2)

There are 3 methods worth describing here:

run - The main loop

This method:

Opens a websocket connecting to the Live API.
Calls the initial setup method.
Then enters the main loop where it alternates between send and recv until send returns False.
The next tutorial will demonstrate how to stream media and run these asynchronously.
send - Sends input text to the api

The send method collects input text from the user, wraps it in a client_content message (an instance of BidiGenerateContentClientContent), and sends it to the model.

If the user sends a q this method returns False to signal that it's time to quit.

recv - Collects audio from the API and plays it

The recv method collects audio chunks in a loop and writes them to a .wav file. It breaks out of the loop once the model sends a turn_complete method, and then plays the audio.

To keep things simple in Colab it collects all the audio before playing it. Other examples demonstrate how to play audio as soon as you start to receive it (using PyAudio), and how to interrupt the model (implement input and audio playback on separate tasks).

Run
Run it:


[ ]
await AudioLoop(['Hello', "What's your name?"]).run()

Working with resumable sessions
Session resumption allows you to return to a previous interaction with the Live API by sending the last session handle you got from the previous session.

When you set your session to be resumable, the session information keeps stored on the Live API for up to 24 hours. In this time window, you can resume the conversation and refer to previous information you have shared with the model.

Helper functions
Start by creating the helper functions for your resumable interaction with the Live API. It will include:


[ ]
import asyncio
import traceback
from asyncio.exceptions import CancelledError

last_handle = None

MODEL =  "gemini-live-2.5-flash-preview"

client = genai.Client(api_key=GOOGLE_API_KEY)

async def async_enumerate(aiterable):
  n=0
  async for item in aiterable:
    yield n, item
    n+=1


def show_response(response):
    new_handle = None
    if text := response.text:
        print(text, end="")
    else:
      print(response.model_dump_json(indent=2, exclude_none=True))
    if response.session_resumption_update:
        new_handle = response.session_resumption_update.new_handle
    return new_handle


async def clock():
  time = 0
  while True:
    await asyncio.sleep(60)
    time += 1
    print(f"{time}:00")


async def recv(session):
  global last_handle
  try:
    while True:
        async for response in session.receive():
            new_handle = show_response(response)
            if new_handle:
                last_handle = new_handle
  except asyncio.CancelledError:
    pass


async def send(session):
  while True:
      message = await asyncio.to_thread(input, "message > ")
      if message.lower() == "q":
          break
      await session.send_client_content(turns={
          'role': 'user',
          'parts': [{'text': message}]
      })


async def async_main(last_handle=None):
  config = types.LiveConnectConfig.model_validate({
      "response_modalities": ["TEXT"],
      "session_resumption": {
          'handle': last_handle,
      }
  })
  try:
    async with (
        client.aio.live.connect(model=MODEL, config=config) as session,
        asyncio.TaskGroup() as tg
    ):
        clock_task = tg.create_task(clock())
        recv_task = tg.create_task(recv(session))
        send_task = tg.create_task(send(session))
        await send_task
        raise asyncio.CancelledError()
  except asyncio.CancelledError:
      pass
  except ExceptionGroup as EG:
      traceback.print_exception(EG)
Now you can start interacting with the Live API (type q to finish the conversation):


[ ]
await async_main()
{
  "session_resumption_update": {}
}
Hello there! How can I help you today?{
  "server_content": {
    "generation_complete": true
  }
}
{
  "server_content": {
    "turn_complete": true
  },
  "usage_metadata": {
    "prompt_token_count": 9,
    "response_token_count": 10,
    "total_token_count": 19,
    "prompt_tokens_details": [
      {
        "modality": "TEXT",
        "token_count": 9
      }
    ],
    "response_tokens_details": [
      {
        "modality": "TEXT",
        "token_count": 10
      }
    ]
  }
}
{
  "session_resumption_update": {
    "new_handle": "Cig2N3lqa3d3MXd4eHFoeDk3cnhmeHUydjlhdHN2cms1bDRnc3c0N2Zq",
    "resumable": true
  }
}
1:00
{
  "session_resumption_update": {}
}
The capital of Brazil is **Brasília**.{
  "server_content": {
    "generation_complete": true
  }
}
{
  "server_content": {
    "turn_complete": true
  },
  "usage_metadata": {
    "prompt_token_count": 36,
    "response_token_count": 9,
    "total_token_count": 45,
    "prompt_tokens_details": [
      {
        "modality": "TEXT",
        "token_count": 36
      }
    ],
    "response_tokens_details": [
      {
        "modality": "TEXT",
        "token_count": 9
      }
    ]
  }
}
{
  "session_resumption_update": {
    "new_handle": "Cig0ZDR1OTViNHVjOWh6aGJvMmhwdWk3NzJiZWRwYW91bnNtajgxZHN1",
    "resumable": true
  }
}
With the session resumption you have the session handle to refer to your previous sessions. In this example, the handle is saved at the last_handle variable as below:


[ ]
last_handle
'Cig0ZDR1OTViNHVjOWh6aGJvMmhwdWk3NzJiZWRwYW91bnNtajgxZHN1'
Now you can start a new Live API session, but this time pointing to a handle from a previous session. Also, to test you could gather information from the previous session, you will ask the model what was the second question you asked before (in this example, it was "what is the capital of Brazil?"). You can see the Live API recovering that information:


[ ]
await async_main(last_handle)
{
  "session_resumption_update": {}
}
The last question you asked was: "what is the capital of brazil?"{
  "server_content": {
    "generation_complete": true
  }
}
{
  "server_content": {
    "turn_complete": true
  },
  "usage_metadata": {
    "prompt_token_count": 63,
    "response_token_count": 15,
    "total_token_count": 78,
    "prompt_tokens_details": [
      {
        "modality": "TEXT",
        "token_count": 63
      }
    ],
    "response_tokens_details": [
      {
        "modality": "TEXT",
        "token_count": 15
      }
    ]
  }
}
{
  "session_resumption_update": {
    "new_handle": "CihyNDg4YTkxanl5cThzYmo4a29lMHRveDJlY3U1amRyNHlqeWF0bWU2",
    "resumable": true
  }
}
Next steps

This tutorial just shows basic usage of the Live API, using the Python GenAI SDK.

If you aren't looking for code, and just want to try multimedia streaming use Live API in Google AI Studio.
If you want to see how to setup streaming interruptible audio and video using the Live API see the Audio and Video input Tutorial.
If you're interested in the low level details of using the websockets directly, see the websocket version of this tutorial.
Try the Tool use in the live API tutorial for an walkthrough of Gemini-2.5's new use capabilities.
There is a Streaming audio in Colab example, but this is more of a demo, it's not optimized for readability.
Other nice Gemini 2.5 examples can also be found in the Cookbook's example directory, in particular the video understanding and the spatial understanding ones.